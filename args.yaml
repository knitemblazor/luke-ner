learning_rate: 0.00001
lr_schedule: "warmup_linear"
weight_decay: 0.01
max_grad_norm: 0.0
adam_b1: 0.9
adam_b2: 0.98
adam_eps: 0.000001
adam_correct_bias: True
warmup_proportion: 0.06
gradient_accumulation_steps: 1
local_rank: -1
fp16: True
fp16_opt_level: "O2"
fp16_min_loss_scale: 1
fp16_max_loss_scale: 4
save_steps: 0
checkpoint_file: "/home/nitheesh/Documents/projects_3/dice_loss_nlp/luke_2/output"
data_dir: "/home/nitheesh/Documents/projects_3/dice_loss_nlp/dice_loss_for_NLP/data"
no-train: True
do_train: True
do_eval: True
no-eval: False
eval_batch_size: 2
max_entity_length: 128
max_mention_length: 30
max_seq_length: 512
no-entity-feature: True
no-word-feature: True
train_batch_size: 4
num_train_epochs: 10
seed: 35
train_on_dev_set: True
tokenizer: ""
model_config: ""
attention_probs_dropout_prob: 0.1
bert_model_name: "roberta-base"
entity_emb_size: 256
entity_vocab_size: 4
finetuning_task: null
hidden_act: "gelu"
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
intermediate_size: 3072
layer_norm_eps: 0.00001
max_position_embeddings: 514
num_attention_heads: 12
num_hidden_layers: 12
num_labels: 2
output_attentions: False
output_hidden_states: False
output_past: True
pruned_heads: {}
torchscript: False
type_vocab_size: 1
use_bfloat16: False
vocab_size: 50265
is_decoder: False
no_word_feature: True
no_entity_feature: True
device: "cpu"
model_path: "/home/nitheesh/Documents/projects_3/dice_loss_nlp/luke_2/luke_base_500k/pytorch_model.bin"
output_dir: "/home/nitheesh/Documents/projects_3/dice_loss_nlp/luke_2/output/"