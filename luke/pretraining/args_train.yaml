dataset_dir: ""
output_dir: ""
multilingual: False
sampling-smoothing: 0.7
parallel: True
cpu: True
bert-model-name: "roberta-large"
entity-emb-size: 256
batch-size: 2048
gradient-accumulation-steps: 1024
learning-rate: 0.00001
lr-schedule: "warmup_linear"
warmup-steps: 2500
adam-b1: 0.9
adam-b2: 0.999
adam-eps: 1e-6
weight-decay: 0.01
max-grad-norm: 0.0
masked-lm-prob: 0.15
masked-entity-prob: 0.15
whole-word-masking/--subword-masking: True
unmasked-word-prob: 0.1
random-word-prob: 0.1
unmasked-entity-prob: 0.0
random-entity-prob: 0.0
mask-words-in-entity-span: True
fix-bert-weights: True
grad-avg-on-cpu: False
num-epochs: 20
global-step: 0
fp16: True
fp16-opt-level: "O2"
fp16-master-weights: True
fp16-min-loss-scale: 1
fp16-max-loss-scale: 4
local-rank", "--local_rank: -1
num-nodes: 1
node-rank: 0
master-addr: "127.0.0.1"
master-port: "29502"
log-dir: None
model-file: None
optimizer-file: None
scheduler-file: None
amp-file: None
save-interval-sec: 1
save-interval-steps: 10
local-rank: 1
args: "{}")